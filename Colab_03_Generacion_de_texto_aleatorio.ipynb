{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RodrigoGuedesDP/Statistics/blob/main/Colab_03_Generacion_de_texto_aleatorio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Generación de texto aleatorio\n",
        "\n",
        "<div>\n",
        "<center>\n",
        "<img src=https://drive.google.com/uc?export=view&id=1NpWawsHQE7YaTtWFlwO4Y0oDkHyb6HFE width=\"500\">\n",
        "</center>\n",
        "</div>\n",
        "\n",
        "\n",
        "### Librería *nltk*\n",
        "\n",
        "https://www.nltk.org/book/"
      ],
      "metadata": {
        "id": "UD8Eqi1ZhbRD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SXidBOpKP7n",
        "outputId": "43989296-fd5d-4e8a-f620-e7fb7dd22177",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('book')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importamos algunos textos"
      ],
      "metadata": {
        "id": "FZzft9Cjhgg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.book import *"
      ],
      "metadata": {
        "id": "JVMVrU5nKTna",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12770fce-e288-4555-e05b-b8b6c682b7bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Introductory Examples for the NLTK Book ***\n",
            "Loading text1, ..., text9 and sent1, ..., sent9\n",
            "Type the name of the text or sentence to view it.\n",
            "Type: 'texts()' or 'sents()' to list the materials.\n",
            "text1: Moby Dick by Herman Melville 1851\n",
            "text2: Sense and Sensibility by Jane Austen 1811\n",
            "text3: The Book of Genesis\n",
            "text4: Inaugural Address Corpus\n",
            "text5: Chat Corpus\n",
            "text6: Monty Python and the Holy Grail\n",
            "text7: Wall Street Journal\n",
            "text8: Personals Corpus\n",
            "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNcHyBCdLgVl",
        "outputId": "fabe51ea-d117-4d45-e8d2-9685218b964f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text1: Moby Dick by Herman Melville 1851\n",
            "text2: Sense and Sensibility by Jane Austen 1811\n",
            "text3: The Book of Genesis\n",
            "text4: Inaugural Address Corpus\n",
            "text5: Chat Corpus\n",
            "text6: Monty Python and the Holy Grail\n",
            "text7: Wall Street Journal\n",
            "text8: Personals Corpus\n",
            "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploramos el texto correspondiente a la novela *Sense and Sensibility*"
      ],
      "metadata": {
        "id": "lmpd5jDphifm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUZhcmdyotaz",
        "outputId": "bac1851f-063f-42b7-eb65-8526495bb1b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Text: Sense and Sensibility by Jane Austen 1811>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(text2)[0:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSHddA6aQ6Pk",
        "outputId": "f94eaf47-0478-4967-bbd4-2318278b1a52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[',\n",
              " 'Sense',\n",
              " 'and',\n",
              " 'Sensibility',\n",
              " 'by',\n",
              " 'Jane',\n",
              " 'Austen',\n",
              " '1811',\n",
              " ']',\n",
              " 'CHAPTER',\n",
              " '1',\n",
              " 'The',\n",
              " 'family',\n",
              " 'of',\n",
              " 'Dashwood',\n",
              " 'had',\n",
              " 'long',\n",
              " 'been',\n",
              " 'settled',\n",
              " 'in']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(text2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGpS8pbWUMPe",
        "outputId": "a0d13cb8-a81f-4825-9384-c269ff90df5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "141576"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtenemos la lista de tokens"
      ],
      "metadata": {
        "id": "zkLyyLVJhphU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words=text2.tokens\n",
        "words[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4yGi02aT8QJ",
        "outputId": "f73b9cc8-798b-4a20-e3fe-9c2deb557226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[',\n",
              " 'Sense',\n",
              " 'and',\n",
              " 'Sensibility',\n",
              " 'by',\n",
              " 'Jane',\n",
              " 'Austen',\n",
              " '1811',\n",
              " ']',\n",
              " 'CHAPTER']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos crear tokens de cualquier texto"
      ],
      "metadata": {
        "id": "PeouGwD-2vnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "oTYrWuI6qktk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(\"Hola buen dia\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdxFpaAbqldL",
        "outputId": "4ab2dc6f-0858-4762-a946-dcb4eafc305c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hola', 'buen', 'dia']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtenemos la frecuencia de cada *token*. *vocab* nos brinda un diccionario de cada *token* y la frecuencia con la cual ocurre."
      ],
      "metadata": {
        "id": "0ASyAfDKhtY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text2.vocab()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrydkTKgXP2b",
        "outputId": "9bc09111-acb8-4c6a-ee59-ee73300087df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({',': 9397, 'to': 4063, '.': 3975, 'the': 3861, 'of': 3565, 'and': 3350, 'her': 2436, 'a': 2043, 'I': 2004, 'in': 1904, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text2.vocab()[\"the\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENwoThvPXV91",
        "outputId": "82513194-d614-4767-aae2-ad55885529c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3861"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cantidad de *tokens* distintos."
      ],
      "metadata": {
        "id": "VGkEgfMJhyHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(set([word.lower() for word in words]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcsY5OosWEKW",
        "outputId": "25720675-6e07-4874-e316-798bc9086529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6403"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cantidad de *tokens* distintos que únicamente tienen letras (omitimos signos de puntuación)"
      ],
      "metadata": {
        "id": "U5TyK6933Vp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(set([word.lower() for word in words if word.isalpha()]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6HBZImOWQ9_",
        "outputId": "3454d41c-4be2-4294-a869-05658c7893ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6283"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lista de palabras (omitimos signos de puntuación) en minúsculas."
      ],
      "metadata": {
        "id": "Xcj7kpg63fSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_texto=[word.lower() for word in words if word.isalpha()]\n",
        "len(words_texto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKW11s3LWfYw",
        "outputId": "f76e2342-0b84-4620-8840-ac7ac5cf1bb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "120733"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_texto[0:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5ndzt5k3wgQ",
        "outputId": "01b9fc19-9852-4a95-b11b-7bf6d042d04f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sense',\n",
              " 'and',\n",
              " 'sensibility',\n",
              " 'by',\n",
              " 'jane',\n",
              " 'austen',\n",
              " 'chapter',\n",
              " 'the',\n",
              " 'family',\n",
              " 'of',\n",
              " 'dashwood',\n",
              " 'had',\n",
              " 'long',\n",
              " 'been',\n",
              " 'settled',\n",
              " 'in',\n",
              " 'sussex',\n",
              " 'their',\n",
              " 'estate',\n",
              " 'was']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frecuencias de estas palabras"
      ],
      "metadata": {
        "id": "zdV5mYA93pUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FreqDist(words_texto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9Egv6i-XB_u",
        "outputId": "44a89264-6b65-4248-cacc-3cdcbd9117fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'to': 4116, 'the': 4105, 'of': 3572, 'and': 3491, 'her': 2551, 'a': 2092, 'i': 2004, 'in': 1979, 'was': 1861, 'it': 1757, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bigramas\n",
        "\n",
        "Un bigrama es un conjunto de dos palabras consecutivas. La función *bigrams* crea una lista de todos los bigramas que aparecen en un texto."
      ],
      "metadata": {
        "id": "X4HXwFSJiHK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(bigrams([\"hola\", \"que\", \"tal\"])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uly8EF4CKY9j",
        "outputId": "d0176f9a-8542-41b7-c55e-7e00a84685bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('hola', 'que'), ('que', 'tal')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigramas=bigrams(words_texto)"
      ],
      "metadata": {
        "id": "OV5yvYTkMdFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Diccionario de frecuencias de bigramas"
      ],
      "metadata": {
        "id": "1KkQ72eBiMfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freq=nltk.FreqDist(bigramas)"
      ],
      "metadata": {
        "id": "K8V8mjj5Xm2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfVHYytVZH0f",
        "outputId": "0e2a51db-771c-4e9e-d6dd-8a429ddbc1e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({('to', 'be'): 436, ('of', 'the'): 430, ('in', 'the'): 359, ('it', 'was'): 280, ('of', 'her'): 276, ('to', 'the'): 242, ('to', 'her'): 230, ('mrs', 'jennings'): 230, ('i', 'am'): 224, ('she', 'was'): 209, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imprimimos aquellos bigramas que empiezan con \"mrs\" y sus frecuencias"
      ],
      "metadata": {
        "id": "Y1xn-UC94Dt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "palabra2=[]\n",
        "pesos=[]\n",
        "for key,value in freq.items():\n",
        "  if key[0]==\"me\":\n",
        "    palabra2.append(key[1])\n",
        "    pesos.append(value)\n",
        "    print(key, value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKGnq_uwZL4p",
        "outputId": "3ca0fefa-2f56-415a-f4f4-fbcae06d0b14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('me', 'replied') 3\n",
            "('me', 'in') 15\n",
            "('me', 'that') 16\n",
            "('me', 'such') 3\n",
            "('me', 'than') 6\n",
            "('me', 'those') 1\n",
            "('me', 'wild') 1\n",
            "('me', 'to') 40\n",
            "('me', 'said') 4\n",
            "('me', 'may') 2\n",
            "('me', 'there') 3\n",
            "('me', 'i') 25\n",
            "('me', 'justice') 1\n",
            "('me', 'it') 12\n",
            "('me', 'a') 14\n",
            "('me', 'out') 3\n",
            "('me', 'yes') 1\n",
            "('me', 'much') 1\n",
            "('me', 'so') 8\n",
            "('me', 'unkindly') 1\n",
            "('me', 'by') 5\n",
            "('me', 'against') 1\n",
            "('me', 'as') 12\n",
            "('me', 'with') 6\n",
            "('me', 'some') 2\n",
            "('me', 'the') 15\n",
            "('me', 'imagine') 1\n",
            "('me', 'of') 12\n",
            "('me', 'but') 7\n",
            "('me', 'easy') 1\n",
            "('me', 'happy') 2\n",
            "('me', 'on') 3\n",
            "('me', 'now') 6\n",
            "('me', 'at') 8\n",
            "('me', 'and') 19\n",
            "('me', 'if') 9\n",
            "('me', 'you') 8\n",
            "('me', 'better') 1\n",
            "('me', 'no') 3\n",
            "('me', 'employment') 1\n",
            "('me', 'any') 3\n",
            "('me', 'what') 7\n",
            "('me', 'this') 3\n",
            "('me', 'about') 1\n",
            "('me', 'he') 5\n",
            "('me', 'for') 11\n",
            "('me', 'very') 2\n",
            "('me', 'word') 1\n",
            "('me', 'otherwise') 2\n",
            "('me', 'then') 1\n",
            "('me', 'impertinently') 1\n",
            "('me', 'was') 4\n",
            "('me', 'miss') 4\n",
            "('me', 'or') 3\n",
            "('me', 'she') 2\n",
            "('me', 'how') 3\n",
            "('me', 'quite') 2\n",
            "('me', 'one') 2\n",
            "('me', 'when') 1\n",
            "('me', 'your') 2\n",
            "('me', 'from') 3\n",
            "('me', 'are') 1\n",
            "('me', 'they') 2\n",
            "('me', 'come') 1\n",
            "('me', 'hear') 2\n",
            "('me', 'whether') 1\n",
            "('me', 'however') 1\n",
            "('me', 'since') 1\n",
            "('me', 'cried') 1\n",
            "('me', 'marianne') 2\n",
            "('me', 'returned') 1\n",
            "('me', 'why') 1\n",
            "('me', 'willoughby') 2\n",
            "('me', 'turned') 1\n",
            "('me', 'tell') 2\n",
            "('me', 'more') 1\n",
            "('me', 'leave') 2\n",
            "('me', 'hate') 2\n",
            "('me', 'forget') 1\n",
            "('me', 'forgive') 1\n",
            "('me', 'throwing') 1\n",
            "('me', 'which') 2\n",
            "('me', 'indeed') 1\n",
            "('me', 'can') 1\n",
            "('me', 'beyond') 1\n",
            "('me', 'ought') 1\n",
            "('me', 'only') 1\n",
            "('me', 'who') 1\n",
            "('me', 'not') 3\n",
            "('me', 'incapable') 1\n",
            "('me', 'would') 1\n",
            "('me', 'occasioned') 1\n",
            "('me', 'though') 1\n",
            "('me', 'brother') 1\n",
            "('me', 'dear') 3\n",
            "('me', 'yesterday') 2\n",
            "('me', 'is') 3\n",
            "('me', 'three') 1\n",
            "('me', 'less') 1\n",
            "('me', 'capable') 1\n",
            "('me', 'entirely') 1\n",
            "('me', 'up') 1\n",
            "('me', 'again') 3\n",
            "('me', 'too') 1\n",
            "('me', 'mrs') 1\n",
            "('me', 'particulars') 1\n",
            "('me', 'kindly') 1\n",
            "('me', 'will') 2\n",
            "('me', 'could') 1\n",
            "('me', 'honestly') 1\n",
            "('me', 'most') 1\n",
            "('me', 'before') 2\n",
            "('me', 'elinor') 1\n",
            "('me', 'perhaps') 1\n",
            "('me', 'almost') 1\n",
            "('me', 'be') 3\n",
            "('me', 'free') 1\n",
            "('me', 'even') 1\n",
            "('me', 'deserved') 1\n",
            "('me', 'oh') 1\n",
            "('me', 'farther') 1\n",
            "('me', 'her') 1\n",
            "('me', 'talking') 1\n",
            "('me', 'know') 1\n",
            "('me', 'asking') 1\n",
            "('me', 'constantly') 1\n",
            "('me', 'make') 1\n",
            "('me', 'an') 1\n",
            "('me', 'already') 1\n",
            "('me', 'ever') 1\n",
            "('me', 'horridly') 1\n",
            "('me', 'acquainted') 1\n",
            "('me', 'added') 1\n",
            "('me', 'appear') 1\n",
            "('me', 'think') 1\n",
            "('me', 'leisure') 1\n",
            "('me', 'regretting') 1\n",
            "('me', 'beg') 1\n",
            "('me', 'great') 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "palabra2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2j9TtG0fHeXQ",
        "outputId": "7bcce5c7-37ff-4ca5-e860-2d4398e28fa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['henry',\n",
              " 'john',\n",
              " 'dashwood',\n",
              " 'ferrars',\n",
              " 'jennings',\n",
              " 'smith',\n",
              " 'palmer',\n",
              " 'jenning',\n",
              " 'taylor',\n",
              " 'ellison',\n",
              " 'brandon',\n",
              " 'willoughby',\n",
              " 'dennison',\n",
              " 'clarke',\n",
              " 'richardson',\n",
              " 'edward',\n",
              " 'mrs',\n",
              " 'robert',\n",
              " 'burgess']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.choices(palabra2, pesos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtiVsLVkhZ8H",
        "outputId": "dbc089df-987f-4bd1-b199-bd2ce8041c60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['jennings']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum=0\n",
        "for key,value in freq.items():\n",
        "  if key[0]==\"mrs\":\n",
        "    sum+=value\n",
        "sum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geUbHeD56rrD",
        "outputId": "f09eeda3-b672-40be-d199-badc22755fde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "530"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distribución empírica"
      ],
      "metadata": {
        "id": "HjPfULykGo-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.array(pesos)/sum"
      ],
      "metadata": {
        "id": "Ag38WCafVar_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e20e5489-95ee-41d3-882e-8f786fbed698"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.00188679, 0.04716981, 0.22830189, 0.14150943, 0.43396226,\n",
              "       0.03207547, 0.07169811, 0.00754717, 0.00377358, 0.00566038,\n",
              "       0.00377358, 0.00566038, 0.00188679, 0.00188679, 0.00377358,\n",
              "       0.00188679, 0.00188679, 0.00377358, 0.00188679])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo de cadena de Markov\n",
        "\n",
        "En un modelo de cadena de Markov, la probabilidad de un texto\n",
        "\n",
        "$$w_1 w_2 ... w_n$$\n",
        "\n",
        "donde $w_i$ es una palabra, se calcula como el producto de la probabilidad de que aparezca la palabra $w_1$ por la probabilidad de transición de la palabra $w_i$ a la palabra $w_{i+1}$ (para $i=1, ..., n-1$).\n",
        "\n",
        "## **Ejercicio 1:**\n",
        "Asumiendo un modelo de cadena de Markov: ¿cuál es la probabilidad de obtener la frase\n",
        "\"mrs dashwood gave me employment\""
      ],
      "metadata": {
        "id": "-Mo_S_UEHJAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generación de texto\n",
        "\n",
        "Texto aleatorio utilizando el modelo de cadena de Markov del lenguage."
      ],
      "metadata": {
        "id": "WbfuZteW4Ly5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generador(palabra_inicial, longitud):\n",
        "  palabra=palabra_inicial\n",
        "  texto_generado=[palabra]\n",
        "  for i in range(longitud):\n",
        "    palabra2=[]\n",
        "    pesos=[]\n",
        "    for key,value in freq.items():\n",
        "      if key[0]==palabra:\n",
        "        palabra2.append(key[1])\n",
        "        pesos.append(value)\n",
        "    palabra=random.choices(palabra2, pesos)[0]\n",
        "    texto_generado.append(palabra)\n",
        "  return texto_generado"
      ],
      "metadata": {
        "id": "xU7DFbXX8kDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generador(\"the\", 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Lk1pwis9plD",
        "outputId": "080432ca-7755-46fd-dbfe-0554b89af7ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'subject',\n",
              " 'would',\n",
              " 'not',\n",
              " 'elinor',\n",
              " 'here',\n",
              " 'and',\n",
              " 'good',\n",
              " 'nature',\n",
              " 'of',\n",
              " 'the']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3-grams\n",
        "\n",
        "Este modelo ya no es una cadena de Markov."
      ],
      "metadata": {
        "id": "64mCnQi9khB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text2.generate(length=30, random_seed=21)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "nCZ4M_xi1L3g",
        "outputId": "2ad56b47-beda-4f11-90d3-7c4d08d9ba70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building ngram index...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "were assured of the different horrors of the two Miss Steeles , lately\n",
            "arrived at their house , was of course , very much to pretend to know\n",
            "the tenderness\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'were assured of the different horrors of the two Miss Steeles , lately\\narrived at their house , was of course , very much to pretend to know\\nthe tenderness'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    }
  ]
}